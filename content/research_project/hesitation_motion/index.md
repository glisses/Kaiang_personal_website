---
title: '[Research Paper] Understanding Cognitive States from Head & Hand Motion Data'

summary: 'This study investigates the potential of machine learning models to predict and interpret nuanced cognitive states such as confusion, hesitation, and readiness in virtual reality (VR) through motion data analysis. By leveraging continuous cognitive annotations, comparing ML performance with human baseline, and employing model visualization techniques, this research aims to uncover fundamental motion patterns underlying cognitive states, paving the way for more adaptive and intelligent VR systems.' 

# Is this a featured talk? (true/false)
featured: false

authors:
- admin

date: '2025-01-15'
# Schedule page publish date (NOT talk date).
publishDate: '2025-01-15T00:00:00Z'

abstract: As virtual reality (VR) and augmented reality (AR) continue to gain popularity, head and hand motion data captured by consumer VR systems have become ubiquitous. Prior work shows such telemetry can be highly identifying and reflect broad user traits, often aligning with intuitive "folk theories" of body language. However, it remains unclear to what extent motion kinematics encode more nuanced cognitive states, such as confusion, hesitation, and readiness, which lack clear correlates with motion. To investigate this, we introduce a novel dataset of head and hand motion with frame-level annotations of these states collected during structured decision-making tasks. Our findings suggest that deep temporal models can infer subtle cognitive states from motion alone, achieving comparable performance with human observers. This work demonstrates that standard VR telemetry contains strong patterns related to users' internal cognitive processes, which opens the door for a new generation of adaptive virtual environments. To enhance reproducibility and support future work, we will make our dataset and modeling framework publicly available.

tags:
- Computer Science - Augmented and Virtual Reality

image:
  caption: ''
  focal_point: Right

links:
#  - icon: twitter
#    icon_pack: fab
#    name: Follow
#    url: https://twitter.com/georgecushen
# url_code: 'https://github.com'
url_pdf: 'https://arxiv.org/abs/2509.24255'
# url_slides: 'https://slideshare.net'
# url_video: 'https://youtube.com'

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
research_projects:
  - MambaReg
---
